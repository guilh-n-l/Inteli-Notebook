# Confusion Matrices

Matrices used to keep track of the classes predicted by the model:

$$
\begin{matrix}
    \, & \, & a & b & c & d\\
    a & \, & 0 & 0 & 0 & 0\\
    b & \, & 0 & 0 & 0 & 0\\ 
    c & \, & 0 & 0 & 0 & 0\\ 
    d & \, & 0 & 0 & 0 & 0\\ 
\end{matrix}
$$
$$
\begin{cases}
    i = j \implies \text{Prediction is correct} \\
    i \ne j \implies \text{Prediction is incorrect}
\end{cases}
$$

## Test metrics

- Accuracy: How well the model can differ between different classes;

$$
\text{acc} = \frac{\text{Correct predictions}}{\text{Total predictions}}
$$

- Recall (Sensitivity): How well the model can generalize a certain class (Higher recall tends to prioritize positives);

$$
\text{rec}_a = \frac{\text{Correct '$a$' predictions}}{\text{'$a$' occurrences}}
$$

- Precision: How well the model can differ a certain class from others (Higher precision tends to decrease **True Positives**); 

$$
\text{prec}_a = \frac{\text{Correct '$a$' predictions}}{\text{Total '$a$' predictions}}
$$

- Specificity: How well the model can avoid false positives;

$$
\text{spec}_a = \frac{\text{Correct non '$a$' predictions}}{\text{non '$a$' occurrences}}
$$

- F-Score: Balance between precision and recall:

$$
F = 2 \cdot \frac{\text{prec}_a \cdot \text{rec}_a}{\text{prec}_a + \text{rec}_a}
$$
