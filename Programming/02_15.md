**Table of Contents**

- [Data pre-processing](#data-pre-processing)
    - [Data labeling](#data-labeling)
    - [Handling certain occurrences](#handling-certain-occurrences)
    - [Choosing features](#choosing-features)
- [Data science terminology](#data-science-terminology)
- [Distance metrics](#distance-metrics)
- [Model testing](#model-testing)
    - [Confusion matrix](#confusion-matrix)
    - [Separating training pool from testing pool](#separating-training-pool-from-testing-pool)

# Data pre-processing

The tasks involved to prepare the data for the processing can be listed below:

- Cleaning: Input of absent values and removal or correction of noisy (contain imprecisions, irrelevant or wrongly inputted data) data;
- Integration: The unification of different databases in a single place;
- Reduction: Removal of redundant or unnecessary data;
- Transformation: Standardization of data to allow usage in various data mining techniques;
- Discretization: Turn continuous values into discrete values to turn them easier to manipulate;

## Data labeling

- One Hot Encoding: When dealing with nominal data, one can avoid the turning them into ordinals by converting the column for n-columns, being 'n' the number of unique elements in the original column;

| Color |
| - |
| Red | 
| Green |
| Blue |
**No encoding**

| Color |
| - |
| 0 | 
| 1 |
| 2 |
**Ordinal encoding**

| Red | Green | Blue |
| - | - | - |
| 1 | 0 | 0 |
| 0 | 1 | 0 |
| 0 | 0 | 1 |
**One Hot encoding**

- Normalization: 

## Handling certain occurrences

1. Absent values:
    - Ignore the object;
    - Input manually by empirical observation;
    - Input the default value;
    - Hot-deck (Inputting the same value from similar objects);
    - Input a value according to past observations;
    - Input median, mode...;
    - Input median, mode... from similar objects;
    - Using other predictive models to choose an input;
2. Noisy data:
    1. Clustering: Group data based in their similarities;
3. Continuous values:
    1. Binning: Group continuous values into groups of discrete values (Ex.: $0 \rightarrow 5'9 \implies \text{short} \,\&\, 5'9 \rightarrow \infty \implies \text{tall}$);
4. Inconsistent data: Choose a standard and change the values accordingly;

## Choosing features

 - F-Classify: Choose best via ANOVA-F value (Calculate difference between the means using F distribution);
 - Chi²: Chooses best via relative difference of categorical values;
 - Recursive Feature Elimination: Select features by recursively considering smaller and smaller sets of features;
 - Random Forrest Classifier: Using trees to select the best features;
 - Pipeline

# Data science terminology

- Bias: The difference between the learning method used and the true relationship of the target with it's features;
- Overfitting: When the disparity between the accuracies when testing the model with the training set and the testing set is too big;

# Distance metrics

- K-Nearest Neighbors: Predict the label of a new data point based on the k-nearest neighbors of that point in the training data (Changing k-values can highly modify the result);
- Decision Tree: Uses conditions chosen by coefficient of Gini to choose the appropriate result;
- Naïve Bayes: Uses independent probabilities to choose the appropriate result;

**_Note:_** When the machine learning method is incapable of capturing the true relationship between the target and the feature, this incapability is called **bias**;



# Model testing

## Confusion matrix

A matrix that helps you to ascertain the balance of the data used in training;

![](img/confusion-matrix.png)

## Separating training pool from testing pool

It's crucial to split the pool of data into 2 separate pools to guaranty the accuracy of your model in the real world during the testing stage

**_Note_:** When using [cross-validation method](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) you can divide your pool into more pools to mix up the training pool