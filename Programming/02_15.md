Table of Contents

- [Data pre-processing](#data-pre-processing)
    - [Handling certain occurrences](#handling-certain-occurrences)
- [Data science terminology](#data-science-terminology)
- [Distance metrics](#distance-metrics)
- [Model testing](#model-testing)
    - [Confusion matrix](#confusion-matrix)
    - [Separating training pool from testing pool](#separating-training-pool-from-testing-pool)


# Data pre-processing

The tasks involved to prepare the data for the processing can be listed below:

- Cleaning: Input of absent values and removal or correction of noisy (contain imprecisions, irrelevant or wrongly inputted data) data;
- Integration: The unification of different databases in a single place;
- Reduction: Removal of redundant or unnecessary data;
- Transformation: Standardization of data to allow usage in various data mining techniques;
- Discretization: Turn continuous values into discrete values to turn them easier to manipulate;

## Handling certain occurrences

1. Absent values:
    - Ignore the object;
    - Input manually by empirical observation;
    - Input the default value;
    - Hot-deck (Inputting the same value from similar objects);
    - Input a value according to past observations;
    - Input median, mode...;
    - Input median, mode... from similar objects;
    - Using other predictive models to choose an input;
2. Noisy data:
    1. Clustering: Group data based in their similarities;
3. Continuous values:
    1. Binning: Group continuous values into groups of discrete values (Ex.: $0 \rightarrow 5'9 \implies \text{short} \,\&\, 5'9 \rightarrow \infty \implies \text{tall}$);
4. Inconsistent data: Choose a standard and change the values accordingly;

# Data science terminology

- Bias: The difference between the learning method used and the true relationship of the target with it's features;
- Overfitting: When the disparity between the accuracies when testing the model with the training set and the testing set is too big;

# Distance metrics

- K-Nearest Neighbors: Predict the label of a new data point based on the k-nearest neighbors of that point in the training data (Changing k-values can highly modify the result);
- Decision Tree: Uses conditions chosen by coefficient of Gini to choose the appropriate result;
- Na√Øve Bayes: Uses independent probabilities to choose the appropriate result;

**_Note:_** When the machine learning method is incapable of capturing the true relationship between the target and the feature, this incapability is called **bias**;



# Model testing

## Confusion matrix

A matrix that helps you to ascertain the balance of the data used in training;

![](img/confusion-matrix.png)

## Separating training pool from testing pool

It's crucial to split the pool of data into 2 separate pools to guaranty the accuracy of your model in the real world during the testing stage

**_Note_:** When using [cross-validation method](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) you can divide your pool into more pools to mix up the training pool